# Global configuration file for DPDK-POT

# All of the configurations related to ingress, egress and transit 
# nodes as well as key management, performance mangement, logging, and other global configurations for 
# the seting up POT communication over multiple containers, or nodes, depending on the use cases, you can
# comment/uncomment configurations below, i.e. if you want container based deployment, you can leave
# it as it, or if you want to deploy multiple nodes over different blocks, you can comment out related
# configurations.

# These settings apply across the entire deployment unless overriden.
global:
  # Initial log level of all components, by changing, you can either modify, the related log
  # level of the components through the docker-compose, or you can set it through the individual
  # DPDK-POT components, on the related nodes, through the CLI tool, that will be available for
  # each of them
  log_level: INFO

  # Base directory on the HOST where generated configs, sockets, etc., might be placed by the prepare 
  # script. The prepare script itself will manage creating subdirectories within this. 
  # Example: /opt/dpdk-pot-deployment  
  deployment_base_dir: "/var/lib/dpdk-pot"

  # Default docker image for all DPDK-POT components. Can be overridden per node, in the node section.
  default_dpdk_pot_image: "dpdk-pot:latest"

  # Default restart policy for docker containers, control how components should beahve when
  # they stop or crash, if container should automatically restart, under what conditions, how
  # the system should handle container failures, etc. (no, always, unless-stopped, on-failure)
  default_restart_policy: "unless-stopped"


  # CPU core allocation strategy for application nodes.
  # The 'prepare' script will use this to assign specific cores.
  # start_core: The first CPU core ID to assign to the first DPDK app container.
  # increment: How many cores to skip for the next container (e.g., 1 for sequential, 2 for 1 core 
  # per container + 1 idle). 
  cpu_allocation:
    start_core: 2

    # Assigns core 2, then 3, then 4...
    increment_per_node: 1   

  # Hugepages configuration (path is for inside the containers, assumed to be mapped from host)
  hugepages:

    # Standard path inside containers
    mount_path: "/dev/hugepages"
    
    # Default DPDK EAL parameters for application nodes (can be overridden per node type/instance)
    default_eal_params:
      mem_channels: 4          # Corresponds to -n EAL option
      socket_mem: "1024,0"     # Corresponds to --socket-mem (e.g., 1GB on NUMA node 0) - often per-node
      no_pci: true             # Corresponds to --no-pci, usually true for vhost-user    

# This defines the overall POT usage, currently the DPDK-POT only supports pipeline structure
# meaning, an ingress node and multiple transit nodes, in between and an egress node, this
# topology specifically can be utilized for Service Function Chaining (SFC) RFC 7665, RFC 8924
topology:
  type: pipeline
  
  # User specifies how many transit nodes they want (e.g., 0, 1, 2, ...)
  number_of_transit_nodes: 2

  # Template for naming transit nodes
  transit_node_naming_tempalte: "transit_node_{index}" 

  # To ensure unique port IDs across nodes
  transit_port_offset: 2

# Configuration for the DPDK application running on the HOST that provides vhost-user sockets.
# Host backend network configurations, there are three or four way of doing this, one is using
# overlay network supplied by docker-compose, another is using testpmd tool supplied by DPDK
# and another is using the OVS-DPDK, as more backend supports are added, this section will be
# updated. By defualt OVS-DPDK will be used.
host_backend:

  # The type of backend to use, "testpmd" or "ovs_dpdk" (or (or "manual" if sockets are pre-configured by 
  # user)
  type: "ovs_dpdk"

  # Base directory on the HOST where OVS will create its vhost-user socket files. OVS typically 
  # manages its sockets in /var/run/openvswitch/ or similar. User can override if they have a 
  # specific setup. Default, ensure OVS has perms here
  socket_base_dir: "/var/run/openvswitch" 


  # ovs_dpdk specific settings
  ovs_dpdk:
    bridge_name: "br-dpdkpot"       # Name of the OVS bridge to be created
    datapath_type: "netdev"         # Standard for DPDK datapath in OVS
    delete_bridge_on_setup: true    # Whether to delete and recreate the bridge on each prepare
  
    # Optional: CPU mask for OVS PMD (Poll Mode Driver) threads. 
    # Example: "0x2" for core 1, "0xc" for cores 2 and 3.
    # If not set, OVS will try to auto-configure or use defaults.
    # pmd_cpu_mask: "0x2"

    # Optional: If you want to bridge any end of the pipeline to a physical DPDK port this requires the 
    # physical NIC to be bound to a DPDK-compatible driver (e.g., vfio-pci) and OVS needs to be 
    # configured to use it.
    # external_interfaces:
    #   ingress_input_from_phy:     # Connects physical NIC to Ingress node's input
    #     phy_port_name: "dpdk0"    # Name OVS will use for the physical port
    #     enabled: false
    #
    #     # Options for OVS to add this port (e.g., pci address) type dpdk 
    #     # options:dpdk-devargs="0000:xx:yy.z"
    #     phy_port_options: "options:dpdk-devargs=0000:03:00.0"
    #
    #   egress_output_to_phy:       # Connects Egress node's output to physical NIC
    #     enabled: false
    #     phy_port_name: "dpdk1"
    #     phy_port_options: "options:dpdk-devargs=0000:03:00.1"    

    # Settings for the vhost-user ports that OVS will create for containers
    vhost_user_port:

      # Number of queues for each vhost-user port created by OVS
      # Should match what the DPDK app in the container expects

      num_queues: 1
      # Other options for vhostuser ports if needed, e.g. options:n_rxq_desc, options:n_txq_desc
      # extra_options: "options:n_rxq_desc=1024 options:n_txq_desc=1024"


# Settings inherited by ingress, transit, and egress
node_common_settings:

  # If image is different from global.default_dpdk_pot_image
  # image_override: "my_special_dpdk_pot_app:v2"

  # For DPDK access. Fine-tune with cap_add if security is paramount.
  privileged_mode: true 

  # cap_add:
  #   - "IPC_LOCK"
  #   - "NET_ADMIN" # If needed for interface setup within container, rare for vhost.

  # Path inside the container where its generated, specific config file will be mounted. The dpdk-pot 
  #application should read its config from this path.
  generated_config_mount_path: "/app/config/node.json"

  # Per-node EAL parameters (can override global.default_eal_params)
  eal:
    mem_channels: 4
    
    # Example: 512MB per application node
    socket_mem: "512,0" 

    # Additional EAL flags for app nodes: e.g. "--log-level=pmd,8"
    extra_flags: "--log-level=pmd,8"

  # Application-specific common settings for dpdk-pot logic
  app_logic:

    # Per-node application log level
    log_level: "info"

    # Default DPDK port configuration within the application
    dpdk_port_config:
      num_rx_queues: 1
      num_tx_queues: 1
      rx_ring_size: 1024
      tx_ring_size: 1024

    # Default mbuf pool configuration within the application
    mbuf_pool_config:
      element_size: 2176      # RTE_MBUF_DEFAULT_BUF_SIZE
      pool_size: 8191         # NUM_MBUFS per pool
      cache_size: 250         # MBUF_CACHE_SIZE
      # num_pools: 1          # If multiple pools are needed per node    

ingress_node:
  # Inherits from node_common_settings. Define overrides or additions here.
  # For this pipeline, typically one ingress instance.
  # eal_override: # If ingress needs different EAL settings than transit/egress
  #   socket_mem: "1024,0"
  
  app_specific_config: # To be written into the node_specific.json for ingress
    # Example:
    # external_interface_mac: "00:11:22:33:44:55" # If ingress captures from a TAP or physical
    pot_profile_name: "ingress_strong_pot" # Reference to a PoT profile defined below
    # Any other parameters specific to ingress logic (e.g., ACLs, rate limits)

transit_node:
  # Inherits from node_common_settings.
  # The 'prepare' script will generate 'pipeline.num_transit_nodes' instances.
  # eal_override:
  #   socket_mem: "256,0" # If transit nodes are lighter
  
  app_specific_config:
    pot_profile_name: "transit_standard_pot"
    # Any other parameters specific to transit logic (e.g., TTL decrement behavior)

egress_node:
  # Inherits from node_common_settings.
  # For this pipeline, typically one egress instance.
  # eal_override:
  #   socket_mem: "1024,0"
  
  app_specific_config:
    pot_profile_name: "egress_validation_pot"
    # Any other parameters specific to egress logic (e.g., reporting, final validation)

# Specifies where the 'prepare' script should look for secret files on the host
# and how they should be named/mounted into containers.
secrets_management:

  # Base path on the HOST where all secret files (like PoT keys) are stored.
  host_secrets_dir: "./pipeline_secrets"

  # Define naming convention for keys if not explicitly path-defined in pot_profiles.
  # pot_key_filename_template: "{node_type}_{profile_name}.key"

  # Path inside containers where secrets directory will be mounted (if mounting whole dir)
  # container_secrets_mount_dir: "/etc/dpdk-pot/secrets"

# Similar to Harbor's _version, used by prepare/migration scripts.
_config_version: "1.0.0"    

